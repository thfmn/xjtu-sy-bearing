{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL-2: LightGBM Baseline for RUL Prediction\n",
    "\n",
    "This notebook implements and evaluates a LightGBM gradient boosting baseline for Remaining Useful Life (RUL) prediction on the XJTU-SY bearing dataset.\n",
    "\n",
    "**Objectives:**\n",
    "1. Train LightGBM on handcrafted features (65 time + frequency domain features)\n",
    "2. Evaluate using leave-one-bearing-out cross-validation\n",
    "3. Extract and visualize feature importance\n",
    "4. Compute SHAP values for interpretability\n",
    "5. Compare with statistical trending baseline (MODEL-1)\n",
    "\n",
    "**Reference baseline from MODEL-1:** RMSE ~31 (RMS threshold baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the pre-extracted features from `outputs/features/features_v2.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "features_df = pd.read_csv('../outputs/features/features_v2.csv')\n",
    "\n",
    "print(f\"Dataset shape: {features_df.shape}\")\n",
    "print(f\"Columns: {features_df.columns.tolist()[:10]}...\")\n",
    "print(f\"\\nSample counts by condition:\")\n",
    "print(features_df.groupby('condition').size())\n",
    "print(f\"\\nBearings: {features_df['bearing_id'].nunique()}\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate feature columns from metadata\n",
    "from src.models.baselines import get_feature_columns\n",
    "\n",
    "feature_cols = get_feature_columns(features_df)\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM Configuration\n",
    "\n",
    "Define hyperparameters for the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.baselines import LGBMConfig, LightGBMBaseline\n",
    "\n",
    "# Configure model\n",
    "config = LGBMConfig(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "print(\"LightGBM Configuration:\")\n",
    "for key, value in config.to_lgb_params().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Training\n",
    "\n",
    "Train with leave-one-bearing-out cross-validation (15 folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.baselines import train_with_cv\n",
    "from src.training.cv import leave_one_bearing_out\n",
    "\n",
    "# Generate CV folds\n",
    "cv_split = leave_one_bearing_out(features_df)\n",
    "print(f\"CV Strategy: {cv_split.strategy}\")\n",
    "print(f\"Number of folds: {len(cv_split)}\")\n",
    "print(f\"\\nFold summary:\")\n",
    "print(cv_split.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with cross-validation\n",
    "cv_results, feature_importance = train_with_cv(\n",
    "    features_df=features_df,\n",
    "    feature_cols=feature_cols,\n",
    "    target_col='rul',\n",
    "    cv_split=cv_split,\n",
    "    config=config,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Results\n",
    "\n",
    "Analyze cross-validation results and compare with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.baselines import evaluate_lgbm_cv\n",
    "\n",
    "# Evaluate CV results\n",
    "eval_results = evaluate_lgbm_cv(cv_results, features_df)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Overall Metrics (All Samples Aggregated)\")\n",
    "print(\"=\" * 60)\n",
    "for metric, value in eval_results['overall'].items():\n",
    "    if 'phm08' in metric:\n",
    "        print(f\"  {metric:25s}: {value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:25s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Cross-Validation Statistics\")\n",
    "print(\"=\" * 60)\n",
    "for metric, value in eval_results['cv_stats'].items():\n",
    "    print(f\"  {metric:25s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-bearing breakdown\n",
    "print(\"\\nPer-Bearing Metrics:\")\n",
    "print(\"=\" * 80)\n",
    "print(eval_results['per_bearing'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with statistical baseline\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparison with Statistical Trending Baseline (MODEL-1)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nStatistical Trending Baselines:\")\n",
    "print(f\"  - RMS Threshold (linear):   RMSE = 30.92\")\n",
    "print(f\"  - Kurtosis Trending:        RMSE = 36.06\")\n",
    "print(f\"  - Health Indicator Fusion:  RMSE = 31.83\")\n",
    "print(f\"\\nLightGBM Baseline:\")\n",
    "print(f\"  - LightGBM CV:              RMSE = {eval_results['overall']['rmse']:.2f}\")\n",
    "print(f\"\\nImprovement over best statistical baseline:\")\n",
    "improvement = 30.92 - eval_results['overall']['rmse']\n",
    "pct_improvement = (improvement / 30.92) * 100\n",
    "print(f\"  - RMSE Reduction: {improvement:.2f} ({pct_improvement:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Identify the most predictive features for RUL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 features by importance\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"=\" * 50)\n",
    "top_20 = feature_importance.head(20)\n",
    "for i, row in top_20.iterrows():\n",
    "    print(f\"  {i+1:2d}. {row['feature']:30s} {row['importance_mean']:10.2f} +/- {row['importance_std']:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "from src.models.baselines import plot_feature_importance\n",
    "\n",
    "fig = plot_feature_importance(feature_importance, top_n=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/lgbm_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group features by domain\n",
    "time_features = [f for f in feature_cols if not any(x in f for x in ['spectral', 'band_power', 'dominant', 'mean_freq'])]\n",
    "freq_features = [f for f in feature_cols if any(x in f for x in ['spectral', 'band_power', 'dominant', 'mean_freq'])]\n",
    "\n",
    "time_importance = feature_importance[feature_importance['feature'].isin(time_features)]['importance_mean'].sum()\n",
    "freq_importance = feature_importance[feature_importance['feature'].isin(freq_features)]['importance_mean'].sum()\n",
    "total_importance = time_importance + freq_importance\n",
    "\n",
    "print(f\"\\nFeature Domain Importance:\")\n",
    "print(f\"  Time-domain features:      {time_importance:,.0f} ({100*time_importance/total_importance:.1f}%)\")\n",
    "print(f\"  Frequency-domain features: {freq_importance:,.0f} ({100*freq_importance/total_importance:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SHAP Value Analysis\n",
    "\n",
    "Compute SHAP values to understand feature contributions to individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single model on all data for SHAP analysis\n",
    "# (CV models are separate, so we train one comprehensive model)\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['rul'].values\n",
    "\n",
    "# Simple train/val split for SHAP demo\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "shap_model = LightGBMBaseline(config)\n",
    "shap_model.fit(X_train, y_train, X_val, y_val, feature_names=feature_cols)\n",
    "print(f\"SHAP model trained. Best iteration: {shap_model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    shap_values, explainer = shap_model.get_shap_values(X_val, max_samples=500)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X_val, feature_names=feature_cols, max_display=20, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/models/lgbm_shap_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Skipping SHAP analysis.\")\n",
    "    print(\"Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP bar plot (mean absolute SHAP value)\n",
    "try:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_val, feature_names=feature_cols, \n",
    "                      max_display=20, plot_type='bar', show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/models/lgbm_shap_bar.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"SHAP not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Visualization\n",
    "\n",
    "Visualize RUL predictions vs ground truth for sample bearings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few bearings to visualize\n",
    "sample_bearings = ['Bearing1_1', 'Bearing2_1', 'Bearing3_1']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, bearing in zip(axes, sample_bearings):\n",
    "    # Find the CV result for this bearing\n",
    "    for result in cv_results:\n",
    "        if bearing in result.val_bearing:\n",
    "            ax.plot(result.y_true, 'b-', label='Ground Truth', linewidth=2)\n",
    "            ax.plot(result.y_pred, 'r--', label='LightGBM Pred', linewidth=2, alpha=0.8)\n",
    "            ax.set_xlabel('Sample Index')\n",
    "            ax.set_ylabel('RUL')\n",
    "            ax.set_title(f'{bearing}\\nRMSE: {result.val_rmse:.2f}')\n",
    "            ax.legend(loc='upper right')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/lgbm_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual\n",
    "all_y_true = np.concatenate([r.y_true for r in cv_results])\n",
    "all_y_pred = np.concatenate([r.y_pred for r in cv_results])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(all_y_true, all_y_pred, alpha=0.3, s=10)\n",
    "ax.plot([0, 125], [0, 125], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax.set_xlabel('Actual RUL', fontsize=12)\n",
    "ax.set_ylabel('Predicted RUL', fontsize=12)\n",
    "ax.set_title(f'LightGBM: Predicted vs Actual RUL\\nRMSE: {eval_results[\"overall\"][\"rmse\"]:.2f}', fontsize=14)\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 130)\n",
    "ax.set_ylim(0, 130)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/lgbm_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **LightGBM Performance**: The LightGBM model achieves competitive RUL prediction on the XJTU-SY dataset using handcrafted features.\n",
    "\n",
    "2. **Feature Importance**: The most important features for RUL prediction are:\n",
    "   - Time-domain: RMS, standard deviation, line integral\n",
    "   - Frequency-domain: Band powers, spectral centroid\n",
    "\n",
    "3. **Comparison with Baseline**: LightGBM provides improvement over pure signal processing baselines by learning non-linear feature interactions.\n",
    "\n",
    "4. **Interpretability**: SHAP values provide insight into which features drive individual predictions, useful for maintenance decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL-2: LightGBM Baseline - Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: {len(features_df)} samples, {len(feature_cols)} features\")\n",
    "print(f\"CV Strategy: Leave-one-bearing-out ({len(cv_split)} folds)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  RMSE:  {eval_results['overall']['rmse']:.2f}\")\n",
    "print(f\"  MAE:   {eval_results['overall']['mae']:.2f}\")\n",
    "print(f\"  MAPE:  {eval_results['overall']['mape']:.1f}%\")\n",
    "print(f\"\\nTop 5 Features:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['feature']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
