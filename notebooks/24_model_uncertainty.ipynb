{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# MODEL-7: Uncertainty Quantification for RUL Prediction\n",
    "\n",
    "This notebook implements uncertainty quantification techniques for Remaining Useful Life (RUL) prediction:\n",
    "\n",
    "1. **Gaussian Output Layer** using `tfp.layers.DistributionLambda` for aleatoric uncertainty\n",
    "2. **Negative Log-Likelihood Loss** for training distributional outputs\n",
    "3. **Monte Carlo Dropout** for epistemic uncertainty estimation\n",
    "4. **Prediction Intervals** visualization on RUL curves\n",
    "5. **Calibration Analysis** to verify uncertainty quality\n",
    "\n",
    "## Uncertainty Types\n",
    "\n",
    "- **Aleatoric Uncertainty**: Irreducible noise in the data (captured by Gaussian output)\n",
    "- **Epistemic Uncertainty**: Model uncertainty due to limited data (captured by MC Dropout)\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Input (spectrograms or raw signals)\n",
    "    ↓\n",
    "Feature Extractor (CNN or TCN)\n",
    "    ↓\n",
    "Gaussian Output Layer (tfp.layers.DistributionLambda)\n",
    "    ↓\n",
    "Normal(μ, σ) → RUL prediction with uncertainty\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Add project root to path\nsys.path.insert(0, os.path.abspath('..'))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\n# Use tf_keras for TensorFlow Probability compatibility\nimport tf_keras as keras\nfrom tf_keras import layers\n\n# Import TensorFlow Probability\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\ntfpl = tfp.layers\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"TensorFlow Probability version: {tfp.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Data and Generate Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "from src.data.loader import XJTUBearingLoader\nfrom src.data.rul_labels import generate_rul_labels\nfrom src.features.stft import extract_spectrogram\n\n# Initialize loader\nloader = XJTUBearingLoader()\nmetadata = loader.get_metadata()\n\nprint(f\"Dataset overview:\")\nprint(f\"  Conditions: {list(metadata.keys())}\")\nprint(f\"  Total bearings: {sum(len(c) for c in metadata.values())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Load data for one bearing\ncondition = '35Hz12kN'\nbearing_id = 'Bearing1_1'\n\nsignals, files = loader.load_bearing(condition, bearing_id)\nprint(f\"{bearing_id}: {len(files)} files\")\n\n# Generate spectrograms\nn_samples = len(signals)\nprint(f\"Generating {n_samples} spectrograms...\")\n\nspectrograms = []\nfor i in range(n_samples):\n    spec = extract_spectrogram(signals[i], sampling_rate=25600.0)\n    spectrograms.append(spec)\n\nX = np.stack(spectrograms).astype(np.float32)\nprint(f\"Spectrograms shape: {X.shape}\")\n\n# Generate RUL labels (using string strategy, not enum)\ny = generate_rul_labels(\n    num_files=len(files),\n    strategy='piecewise_linear',\n    max_rul=125\n).astype(np.float32)\n\nprint(f\"RUL range: {y.min():.1f} - {y.max():.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation: {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Gaussian Output Layer with TensorFlow Probability\n",
    "\n",
    "We implement a Gaussian output layer using `tfp.layers.DistributionLambda` that outputs a Normal distribution.\n",
    "The network learns both the mean (μ) and standard deviation (σ) of the RUL prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gaussian_output_model(\n",
    "    input_shape=(128, 128, 2),\n",
    "    base_filters=32,\n",
    "    num_blocks=4,\n",
    "):\n",
    "    \"\"\"Build CNN model with Gaussian output layer for uncertainty quantification.\n",
    "    \n",
    "    The model outputs a Normal distribution N(μ, σ) where:\n",
    "    - μ (mean) is the predicted RUL\n",
    "    - σ (std) represents the aleatoric uncertainty\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input spectrograms.\n",
    "        base_filters: Base number of filters for CNN.\n",
    "        num_blocks: Number of convolutional blocks.\n",
    "        \n",
    "    Returns:\n",
    "        Keras model outputting a TFP distribution.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape, name='spectrogram_input')\n",
    "    \n",
    "    # CNN backbone\n",
    "    x = inputs\n",
    "    for i in range(num_blocks):\n",
    "        filters = base_filters * (2 ** i)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output 2 parameters: mean and log(variance)\n",
    "    # Using log(variance) ensures variance is always positive after exp()\n",
    "    params = layers.Dense(2, name='distribution_params')(x)\n",
    "    \n",
    "    # Create Gaussian distribution using DistributionLambda\n",
    "    # The lambda splits params into mean and std\n",
    "    distribution = tfpl.DistributionLambda(\n",
    "        lambda t: tfd.Normal(\n",
    "            loc=tf.nn.relu(t[..., :1]),  # Mean (non-negative via ReLU)\n",
    "            scale=tf.nn.softplus(t[..., 1:]) + 1e-6  # Std (positive via softplus)\n",
    "        ),\n",
    "        name='gaussian_output'\n",
    "    )(params)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=distribution, name='gaussian_rul_model')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the model\n",
    "gaussian_model = build_gaussian_output_model()\n",
    "gaussian_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Negative Log-Likelihood Loss\n",
    "\n",
    "For distributional outputs, we use Negative Log-Likelihood (NLL) as the loss function:\n",
    "\n",
    "$$\\mathcal{L}_{NLL} = -\\log p(y | \\mu, \\sigma) = \\frac{1}{2}\\log(2\\pi\\sigma^2) + \\frac{(y - \\mu)^2}{2\\sigma^2}$$\n",
    "\n",
    "This loss encourages the model to:\n",
    "1. Predict accurate mean values (μ close to true y)\n",
    "2. Estimate appropriate uncertainty (σ matches prediction error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(y_true, y_pred_distribution):\n",
    "    \"\"\"Negative log-likelihood loss for distributional output.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True RUL values, shape (batch, 1).\n",
    "        y_pred_distribution: TFP distribution object.\n",
    "        \n",
    "    Returns:\n",
    "        Scalar loss value.\n",
    "    \"\"\"\n",
    "    return -tf.reduce_mean(y_pred_distribution.log_prob(y_true))\n",
    "\n",
    "\n",
    "# Compile model with NLL loss\n",
    "gaussian_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=negative_log_likelihood,\n",
    ")\n",
    "\n",
    "print(\"Model compiled with Negative Log-Likelihood loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Gaussian output model\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Reshape y for training\n",
    "y_train_reshaped = y_train.reshape(-1, 1)\n",
    "y_val_reshaped = y_val.reshape(-1, 1)\n",
    "\n",
    "history_gaussian = gaussian_model.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(history_gaussian.history['loss'], label='Train NLL')\n",
    "ax.plot(history_gaussian.history['val_loss'], label='Val NLL')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Negative Log-Likelihood')\n",
    "ax.set_title('Gaussian Model Training History')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/uncertainty_training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo Dropout for Epistemic Uncertainty\n",
    "\n",
    "Monte Carlo Dropout estimates epistemic uncertainty by:\n",
    "1. Keeping dropout active during inference\n",
    "2. Running multiple forward passes\n",
    "3. Computing mean and variance of predictions\n",
    "\n",
    "This captures model uncertainty due to limited training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mc_dropout_model(\n",
    "    input_shape=(128, 128, 2),\n",
    "    base_filters=32,\n",
    "    num_blocks=4,\n",
    "    dropout_rate=0.2,\n",
    "):\n",
    "    \"\"\"Build CNN model with MC Dropout for epistemic uncertainty.\n",
    "    \n",
    "    Uses dropout layers that remain active during inference (training=True).\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input spectrograms.\n",
    "        base_filters: Base number of filters for CNN.\n",
    "        num_blocks: Number of convolutional blocks.\n",
    "        dropout_rate: Dropout rate for uncertainty estimation.\n",
    "        \n",
    "    Returns:\n",
    "        Keras model with dropout.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape, name='spectrogram_input')\n",
    "    \n",
    "    # CNN backbone with dropout after each block\n",
    "    x = inputs\n",
    "    for i in range(num_blocks):\n",
    "        filters = base_filters * (2 ** i)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)  # Spatial dropout\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense layers with dropout\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output (non-negative RUL)\n",
    "    outputs = layers.Dense(1, activation='relu', name='rul_output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='mc_dropout_model')\n",
    "    return model\n",
    "\n",
    "\n",
    "def mc_dropout_predict(model, X, n_samples=50):\n",
    "    \"\"\"Make predictions with MC Dropout uncertainty estimation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model with dropout layers.\n",
    "        X: Input data.\n",
    "        n_samples: Number of stochastic forward passes.\n",
    "        \n",
    "    Returns:\n",
    "        mean: Mean prediction.\n",
    "        std: Standard deviation (epistemic uncertainty).\n",
    "        all_preds: All individual predictions.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Set training=True to keep dropout active\n",
    "        pred = model(X, training=True)\n",
    "        predictions.append(pred.numpy())\n",
    "    \n",
    "    all_preds = np.stack(predictions, axis=0)  # (n_samples, batch, 1)\n",
    "    mean = np.mean(all_preds, axis=0)\n",
    "    std = np.std(all_preds, axis=0)\n",
    "    \n",
    "    return mean, std, all_preds\n",
    "\n",
    "\n",
    "# Build MC Dropout model\n",
    "mc_model = build_mc_dropout_model(dropout_rate=0.3)\n",
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MC Dropout model\n",
    "from src.training.config import TrainingConfig, compile_model\n",
    "\n",
    "config = TrainingConfig(\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    ")\n",
    "compile_model(mc_model, config)\n",
    "\n",
    "history_mc = mc_model.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Dropout predictions\n",
    "print(\"Running MC Dropout inference (50 forward passes)...\")\n",
    "mc_mean, mc_std, mc_all_preds = mc_dropout_predict(mc_model, X_val, n_samples=50)\n",
    "\n",
    "print(f\"MC Dropout Results:\")\n",
    "print(f\"  Mean prediction shape: {mc_mean.shape}\")\n",
    "print(f\"  Std (uncertainty) shape: {mc_std.shape}\")\n",
    "print(f\"  Average uncertainty: {mc_std.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Visualize Prediction Intervals on RUL Curves\n",
    "\n",
    "We visualize the predictions with uncertainty bands to show:\n",
    "- Mean prediction (solid line)\n",
    "- 68% confidence interval (1σ band)\n",
    "- 95% confidence interval (2σ band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Gaussian model predictions\n",
    "gaussian_dist = gaussian_model(X_val)\n",
    "gaussian_mean = gaussian_dist.mean().numpy()\n",
    "gaussian_std = gaussian_dist.stddev().numpy()\n",
    "\n",
    "print(f\"Gaussian Model Results:\")\n",
    "print(f\"  Mean prediction shape: {gaussian_mean.shape}\")\n",
    "print(f\"  Std (aleatoric) shape: {gaussian_std.shape}\")\n",
    "print(f\"  Average uncertainty: {gaussian_std.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rul_with_uncertainty(\n",
    "    y_true, y_pred_mean, y_pred_std, \n",
    "    title='RUL Prediction with Uncertainty',\n",
    "    ax=None\n",
    "):\n",
    "    \"\"\"Plot RUL predictions with uncertainty bands.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True RUL values.\n",
    "        y_pred_mean: Predicted mean RUL.\n",
    "        y_pred_std: Predicted standard deviation.\n",
    "        title: Plot title.\n",
    "        ax: Matplotlib axis (creates new if None).\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Sort by true RUL for cleaner visualization\n",
    "    sort_idx = np.argsort(y_true.flatten())\n",
    "    y_true_sorted = y_true.flatten()[sort_idx]\n",
    "    y_mean_sorted = y_pred_mean.flatten()[sort_idx]\n",
    "    y_std_sorted = y_pred_std.flatten()[sort_idx]\n",
    "    \n",
    "    x = np.arange(len(y_true_sorted))\n",
    "    \n",
    "    # Plot ground truth\n",
    "    ax.plot(x, y_true_sorted, 'k-', linewidth=2, label='Ground Truth', alpha=0.8)\n",
    "    \n",
    "    # Plot mean prediction\n",
    "    ax.plot(x, y_mean_sorted, 'b-', linewidth=2, label='Predicted Mean')\n",
    "    \n",
    "    # 68% confidence interval (1σ)\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        y_mean_sorted - y_std_sorted,\n",
    "        y_mean_sorted + y_std_sorted,\n",
    "        alpha=0.3,\n",
    "        color='blue',\n",
    "        label='68% CI (1σ)'\n",
    "    )\n",
    "    \n",
    "    # 95% confidence interval (2σ)\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        y_mean_sorted - 1.96 * y_std_sorted,\n",
    "        y_mean_sorted + 1.96 * y_std_sorted,\n",
    "        alpha=0.15,\n",
    "        color='blue',\n",
    "        label='95% CI (2σ)'\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Sample Index (sorted by true RUL)')\n",
    "    ax.set_ylabel('RUL')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "plot_rul_with_uncertainty(\n",
    "    y_val_reshaped, gaussian_mean, gaussian_std,\n",
    "    title='Gaussian Output Model (Aleatoric Uncertainty)',\n",
    "    ax=axes[0]\n",
    ")\n",
    "\n",
    "plot_rul_with_uncertainty(\n",
    "    y_val_reshaped, mc_mean, mc_std,\n",
    "    title='MC Dropout Model (Epistemic Uncertainty)',\n",
    "    ax=axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/uncertainty_prediction_intervals.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot uncertainty vs RUL position (expectation: higher uncertainty near end-of-life)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gaussian model\n",
    "axes[0].scatter(y_val_reshaped, gaussian_std, alpha=0.6, c='blue')\n",
    "axes[0].set_xlabel('True RUL')\n",
    "axes[0].set_ylabel('Predicted Std (Uncertainty)')\n",
    "axes[0].set_title('Gaussian Model: Uncertainty vs RUL')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(y_val_reshaped.flatten(), gaussian_std.flatten(), 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(y_val.min(), y_val.max(), 100)\n",
    "axes[0].plot(x_line, p(x_line), 'r--', linewidth=2, label=f'Trend (slope={z[0]:.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# MC Dropout model\n",
    "axes[1].scatter(y_val_reshaped, mc_std, alpha=0.6, c='green')\n",
    "axes[1].set_xlabel('True RUL')\n",
    "axes[1].set_ylabel('MC Dropout Std (Uncertainty)')\n",
    "axes[1].set_title('MC Dropout Model: Uncertainty vs RUL')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z2 = np.polyfit(y_val_reshaped.flatten(), mc_std.flatten(), 1)\n",
    "p2 = np.poly1d(z2)\n",
    "axes[1].plot(x_line, p2(x_line), 'r--', linewidth=2, label=f'Trend (slope={z2[0]:.4f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/uncertainty_vs_rul.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nUncertainty Trend Analysis:\")\n",
    "print(f\"  Gaussian: slope = {z[0]:.4f} (negative = higher uncertainty at low RUL)\")\n",
    "print(f\"  MC Dropout: slope = {z2[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 6. Calibration Analysis\n",
    "\n",
    "A well-calibrated uncertainty estimate should have:\n",
    "- ~68% of true values within 1σ interval\n",
    "- ~95% of true values within 2σ interval\n",
    "\n",
    "We plot a calibration curve comparing expected vs. observed coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calibration_curve(y_true, y_pred_mean, y_pred_std, n_bins=10):\n",
    "    \"\"\"Compute calibration curve for uncertainty estimates.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values.\n",
    "        y_pred_mean: Predicted mean.\n",
    "        y_pred_std: Predicted standard deviation.\n",
    "        n_bins: Number of confidence levels to evaluate.\n",
    "        \n",
    "    Returns:\n",
    "        expected_coverage: Expected coverage percentages.\n",
    "        observed_coverage: Observed coverage percentages.\n",
    "    \"\"\"\n",
    "    # Confidence levels from 0.1 to 0.99\n",
    "    confidence_levels = np.linspace(0.1, 0.99, n_bins)\n",
    "    \n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_mean_flat = y_pred_mean.flatten()\n",
    "    y_std_flat = y_pred_std.flatten()\n",
    "    \n",
    "    observed_coverage = []\n",
    "    \n",
    "    for conf in confidence_levels:\n",
    "        # Calculate z-score for this confidence level\n",
    "        from scipy import stats\n",
    "        z = stats.norm.ppf((1 + conf) / 2)\n",
    "        \n",
    "        # Check what fraction of true values fall within the interval\n",
    "        lower = y_mean_flat - z * y_std_flat\n",
    "        upper = y_mean_flat + z * y_std_flat\n",
    "        \n",
    "        within_interval = (y_true_flat >= lower) & (y_true_flat <= upper)\n",
    "        observed = np.mean(within_interval)\n",
    "        observed_coverage.append(observed)\n",
    "    \n",
    "    return confidence_levels, np.array(observed_coverage)\n",
    "\n",
    "\n",
    "# Compute calibration for both models\n",
    "conf_levels, obs_gaussian = compute_calibration_curve(\n",
    "    y_val_reshaped, gaussian_mean, gaussian_std, n_bins=15\n",
    ")\n",
    "_, obs_mc = compute_calibration_curve(\n",
    "    y_val_reshaped, mc_mean, mc_std, n_bins=15\n",
    ")\n",
    "\n",
    "print(\"Calibration Analysis Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curves\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Perfect calibration line\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')\n",
    "\n",
    "# Gaussian model calibration\n",
    "ax.plot(conf_levels, obs_gaussian, 'b-o', linewidth=2, markersize=6, \n",
    "        label='Gaussian Output')\n",
    "\n",
    "# MC Dropout calibration\n",
    "ax.plot(conf_levels, obs_mc, 'g-s', linewidth=2, markersize=6,\n",
    "        label='MC Dropout')\n",
    "\n",
    "ax.set_xlabel('Expected Coverage (Confidence Level)', fontsize=12)\n",
    "ax.set_ylabel('Observed Coverage', fontsize=12)\n",
    "ax.set_title('Uncertainty Calibration Plot', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Add calibration error annotation\n",
    "ece_gaussian = np.mean(np.abs(conf_levels - obs_gaussian))\n",
    "ece_mc = np.mean(np.abs(conf_levels - obs_mc))\n",
    "\n",
    "ax.text(0.05, 0.90, f'ECE (Gaussian): {ece_gaussian:.3f}', transform=ax.transAxes, fontsize=11)\n",
    "ax.text(0.05, 0.85, f'ECE (MC Dropout): {ece_mc:.3f}', transform=ax.transAxes, fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/uncertainty_calibration.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExpected Calibration Error (ECE):\")\n",
    "print(f\"  Gaussian Model: {ece_gaussian:.4f}\")\n",
    "print(f\"  MC Dropout Model: {ece_mc:.4f}\")\n",
    "print(f\"  (Lower ECE = better calibrated uncertainty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check specific coverage rates\n",
    "from scipy import stats\n",
    "\n",
    "def check_coverage(y_true, y_mean, y_std, confidence_levels=[0.68, 0.95, 0.99]):\n",
    "    \"\"\"Check coverage at specific confidence levels.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_mean_flat = y_mean.flatten()\n",
    "    y_std_flat = y_std.flatten()\n",
    "    \n",
    "    for conf in confidence_levels:\n",
    "        z = stats.norm.ppf((1 + conf) / 2)\n",
    "        lower = y_mean_flat - z * y_std_flat\n",
    "        upper = y_mean_flat + z * y_std_flat\n",
    "        within = np.mean((y_true_flat >= lower) & (y_true_flat <= upper))\n",
    "        results.append({\n",
    "            'expected': conf,\n",
    "            'observed': within,\n",
    "            'gap': within - conf\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Gaussian Model Coverage:\")\n",
    "gaussian_coverage = check_coverage(y_val_reshaped, gaussian_mean, gaussian_std)\n",
    "print(gaussian_coverage.to_string(index=False))\n",
    "\n",
    "print(\"\\nMC Dropout Model Coverage:\")\n",
    "mc_coverage = check_coverage(y_val_reshaped, mc_mean, mc_std)\n",
    "print(mc_coverage.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 7. Combined Uncertainty (Aleatoric + Epistemic)\n",
    "\n",
    "For the most comprehensive uncertainty estimate, we can combine both types:\n",
    "\n",
    "$$\\sigma_{total}^2 = \\sigma_{aleatoric}^2 + \\sigma_{epistemic}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_uncertainty_model(\n",
    "    input_shape=(128, 128, 2),\n",
    "    base_filters=32,\n",
    "    num_blocks=4,\n",
    "    dropout_rate=0.2,\n",
    "):\n",
    "    \"\"\"Build model that outputs Gaussian params with dropout for combined uncertainty.\n",
    "    \n",
    "    Combines:\n",
    "    - Aleatoric uncertainty via Gaussian output (learned σ)\n",
    "    - Epistemic uncertainty via MC Dropout (variance of predictions)\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape, name='spectrogram_input')\n",
    "    \n",
    "    # CNN backbone with dropout\n",
    "    x = inputs\n",
    "    for i in range(num_blocks):\n",
    "        filters = base_filters * (2 ** i)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output distribution parameters\n",
    "    params = layers.Dense(2, name='distribution_params')(x)\n",
    "    \n",
    "    # Gaussian distribution\n",
    "    distribution = tfpl.DistributionLambda(\n",
    "        lambda t: tfd.Normal(\n",
    "            loc=tf.nn.relu(t[..., :1]),\n",
    "            scale=tf.nn.softplus(t[..., 1:]) + 1e-6\n",
    "        ),\n",
    "        name='gaussian_output'\n",
    "    )(params)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=distribution, name='combined_uncertainty_model')\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_with_combined_uncertainty(model, X, n_mc_samples=50):\n",
    "    \"\"\"Predict with combined aleatoric and epistemic uncertainty.\n",
    "    \n",
    "    Returns:\n",
    "        mean: Mean prediction.\n",
    "        aleatoric_std: Aleatoric uncertainty (average learned σ).\n",
    "        epistemic_std: Epistemic uncertainty (std of means across MC samples).\n",
    "        total_std: Combined uncertainty.\n",
    "    \"\"\"\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "    \n",
    "    for _ in range(n_mc_samples):\n",
    "        dist = model(X, training=True)\n",
    "        all_means.append(dist.mean().numpy())\n",
    "        all_stds.append(dist.stddev().numpy())\n",
    "    \n",
    "    all_means = np.stack(all_means, axis=0)  # (n_mc, batch, 1)\n",
    "    all_stds = np.stack(all_stds, axis=0)\n",
    "    \n",
    "    # Mean prediction\n",
    "    mean = np.mean(all_means, axis=0)\n",
    "    \n",
    "    # Aleatoric uncertainty: average of learned stds\n",
    "    aleatoric_std = np.mean(all_stds, axis=0)\n",
    "    \n",
    "    # Epistemic uncertainty: std of means across MC samples\n",
    "    epistemic_std = np.std(all_means, axis=0)\n",
    "    \n",
    "    # Total uncertainty: sqrt(aleatoric^2 + epistemic^2)\n",
    "    total_std = np.sqrt(aleatoric_std**2 + epistemic_std**2)\n",
    "    \n",
    "    return mean, aleatoric_std, epistemic_std, total_std\n",
    "\n",
    "\n",
    "# Build and train combined model\n",
    "combined_model = build_combined_uncertainty_model(dropout_rate=0.25)\n",
    "combined_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=negative_log_likelihood,\n",
    ")\n",
    "\n",
    "print(\"Combined Uncertainty Model:\")\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train combined model\n",
    "history_combined = combined_model.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get combined uncertainty predictions\n",
    "print(\"Computing combined uncertainty (50 MC samples)...\")\n",
    "comb_mean, comb_aleatoric, comb_epistemic, comb_total = predict_with_combined_uncertainty(\n",
    "    combined_model, X_val, n_mc_samples=50\n",
    ")\n",
    "\n",
    "print(f\"\\nCombined Uncertainty Results:\")\n",
    "print(f\"  Mean aleatoric uncertainty: {comb_aleatoric.mean():.4f}\")\n",
    "print(f\"  Mean epistemic uncertainty: {comb_epistemic.mean():.4f}\")\n",
    "print(f\"  Mean total uncertainty: {comb_total.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty decomposition\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sort by true RUL\n",
    "sort_idx = np.argsort(y_val_reshaped.flatten())\n",
    "y_sorted = y_val_reshaped.flatten()[sort_idx]\n",
    "aleatoric_sorted = comb_aleatoric.flatten()[sort_idx]\n",
    "epistemic_sorted = comb_epistemic.flatten()[sort_idx]\n",
    "total_sorted = comb_total.flatten()[sort_idx]\n",
    "\n",
    "x = np.arange(len(y_sorted))\n",
    "\n",
    "# Stacked area plot of uncertainties\n",
    "axes[0].fill_between(x, 0, aleatoric_sorted, alpha=0.6, label='Aleatoric', color='blue')\n",
    "axes[0].fill_between(x, aleatoric_sorted, aleatoric_sorted + epistemic_sorted, \n",
    "                     alpha=0.6, label='Epistemic', color='orange')\n",
    "axes[0].set_xlabel('Sample Index (sorted by RUL)')\n",
    "axes[0].set_ylabel('Uncertainty (Std)')\n",
    "axes[0].set_title('Uncertainty Decomposition')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: Aleatoric vs Epistemic\n",
    "axes[1].scatter(comb_aleatoric, comb_epistemic, c=y_val_reshaped, cmap='coolwarm', alpha=0.7)\n",
    "axes[1].set_xlabel('Aleatoric Uncertainty')\n",
    "axes[1].set_ylabel('Epistemic Uncertainty')\n",
    "axes[1].set_title('Aleatoric vs Epistemic (color=RUL)')\n",
    "cbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "cbar.set_label('True RUL')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Total uncertainty vs RUL\n",
    "axes[2].scatter(y_val_reshaped, comb_total, alpha=0.6, c='purple')\n",
    "axes[2].set_xlabel('True RUL')\n",
    "axes[2].set_ylabel('Total Uncertainty')\n",
    "axes[2].set_title('Total Uncertainty vs RUL')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(y_val_reshaped.flatten(), comb_total.flatten(), 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(y_val.min(), y_val.max(), 100)\n",
    "axes[2].plot(x_line, p(x_line), 'r--', linewidth=2, label=f'Trend (slope={z[0]:.4f})')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/models/uncertainty_decomposition.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 8. Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.metrics import rmse, mae, phm08_score\n",
    "\n",
    "# Compute metrics for all models\n",
    "results = []\n",
    "\n",
    "# Gaussian model\n",
    "results.append({\n",
    "    'Model': 'Gaussian Output',\n",
    "    'RMSE': rmse(y_val_reshaped.flatten(), gaussian_mean.flatten()),\n",
    "    'MAE': mae(y_val_reshaped.flatten(), gaussian_mean.flatten()),\n",
    "    'Mean Uncertainty': gaussian_std.mean(),\n",
    "    'ECE': ece_gaussian,\n",
    "})\n",
    "\n",
    "# MC Dropout model\n",
    "results.append({\n",
    "    'Model': 'MC Dropout',\n",
    "    'RMSE': rmse(y_val_reshaped.flatten(), mc_mean.flatten()),\n",
    "    'MAE': mae(y_val_reshaped.flatten(), mc_mean.flatten()),\n",
    "    'Mean Uncertainty': mc_std.mean(),\n",
    "    'ECE': ece_mc,\n",
    "})\n",
    "\n",
    "# Combined model\n",
    "_, obs_combined = compute_calibration_curve(\n",
    "    y_val_reshaped, comb_mean, comb_total, n_bins=15\n",
    ")\n",
    "ece_combined = np.mean(np.abs(conf_levels - obs_combined))\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Combined (Ale+Epi)',\n",
    "    'RMSE': rmse(y_val_reshaped.flatten(), comb_mean.flatten()),\n",
    "    'MAE': mae(y_val_reshaped.flatten(), comb_mean.flatten()),\n",
    "    'Mean Uncertainty': comb_total.mean(),\n",
    "    'ECE': ece_combined,\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNCERTAINTY QUANTIFICATION MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save results\n",
    "os.makedirs('../outputs/evaluation', exist_ok=True)\n",
    "results_df.to_csv('../outputs/evaluation/uncertainty_comparison.csv', index=False)\n",
    "print(\"\\nResults saved to outputs/evaluation/uncertainty_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Gaussian Output Layer** (tfp.layers.DistributionLambda)\n",
    "   - Successfully learns aleatoric uncertainty alongside mean prediction\n",
    "   - Trained with negative log-likelihood loss\n",
    "   - Provides per-sample uncertainty estimates\n",
    "\n",
    "2. **Monte Carlo Dropout**\n",
    "   - Captures epistemic uncertainty via multiple stochastic forward passes\n",
    "   - No architectural changes needed (just keep dropout active)\n",
    "   - Computational cost scales with number of MC samples\n",
    "\n",
    "3. **Uncertainty Behavior**\n",
    "   - Expected: Higher uncertainty near end-of-life (low RUL)\n",
    "   - Observed: Trend analysis shows uncertainty patterns across RUL range\n",
    "   \n",
    "4. **Calibration Quality**\n",
    "   - Expected Calibration Error (ECE) measures reliability\n",
    "   - Lower ECE indicates better-calibrated uncertainty\n",
    "   - 68% CI should contain ~68% of true values\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **For Production**: Use combined uncertainty (aleatoric + epistemic)\n",
    "2. **For Speed**: Use Gaussian output alone (single forward pass)\n",
    "3. **For Robustness**: Use MC Dropout with 20-50 samples\n",
    "4. **Always**: Validate calibration on held-out data\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Apply uncertainty models to full dataset with cross-validation\n",
    "2. Integrate uncertainty into maintenance decision-making\n",
    "3. Explore deep ensembles for additional diversity\n",
    "4. Implement temperature scaling for calibration improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "os.makedirs('../outputs/models', exist_ok=True)\n",
    "\n",
    "gaussian_model.save('../outputs/models/gaussian_uncertainty_model.keras')\n",
    "mc_model.save('../outputs/models/mc_dropout_model.keras')\n",
    "combined_model.save('../outputs/models/combined_uncertainty_model.keras')\n",
    "\n",
    "print(\"Models saved to outputs/models/\")\n",
    "print(\"  - gaussian_uncertainty_model.keras\")\n",
    "print(\"  - mc_dropout_model.keras\")\n",
    "print(\"  - combined_uncertainty_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}