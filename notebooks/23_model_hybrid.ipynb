{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Hybrid Architecture Experiments (MODEL-6)\n",
    "\n",
    "This notebook explores hybrid architectures that combine different model paradigms for RUL prediction.\n",
    "\n",
    "## Experiments\n",
    "\n",
    "1. **TCN -> Transformer Hybrid**: Use TCN for local feature extraction followed by Transformer for global context\n",
    "2. **2D CNN + State Space Model (SSM)**: Use CNN for spectrogram features combined with SSM for temporal modeling\n",
    "\n",
    "## Architecture Comparisons\n",
    "\n",
    "| Hybrid | Feature Extractor | Temporal Model | Input Type |\n",
    "|--------|------------------|----------------|------------|\n",
    "| TCN-Transformer | Multi-scale TCN | Transformer Encoder | Raw 1D signals |\n",
    "| CNN-SSM | 2D CNN Backbone | State Space Model | Spectrograms |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Optional, List\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import existing model components\n",
    "from src.models.pattern1 import (\n",
    "    create_tcn_transformer_lstm,\n",
    "    create_tcn_transformer_transformer,\n",
    "    StemConfig,\n",
    "    TCNConfig,\n",
    "    print_model_summary as print_model_summary_p1,\n",
    ")\n",
    "from src.models.pattern1.stem import DualChannelStem\n",
    "from src.models.pattern1.tcn import DualChannelTCN, TCNEncoder\n",
    "from src.models.pattern1.attention import BidirectionalCrossAttention, AttentionConfig\n",
    "from src.models.pattern1.aggregator import TransformerAggregator, TransformerAggregatorConfig\n",
    "from src.models.pattern1.model import RULHead, TemporalDownsampler\n",
    "\n",
    "from src.models.pattern2 import (\n",
    "    create_pattern2_lstm,\n",
    "    create_pattern2_transformer,\n",
    "    CNN2DBackboneConfig,\n",
    "    print_model_summary as print_model_summary_p2,\n",
    ")\n",
    "from src.models.pattern2.backbone import DualChannelCNN2DBackbone\n",
    "from src.models.pattern2.model import LateFusion\n",
    "\n",
    "from src.training.config import TrainingConfig, compile_model, build_callbacks\n",
    "from src.training.metrics import rmse, mae, phm08_score\n",
    "\n",
    "from src.data.loader import XJTUBearingLoader\n",
    "from src.data.rul_labels import generate_rul_labels\n",
    "from src.features.stft import extract_spectrogram\n",
    "\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hybrid 1: TCN -> Transformer\n",
    "\n",
    "This hybrid combines the best of both worlds:\n",
    "- **TCN**: Excellent at capturing local patterns with multi-scale receptive fields via dilated convolutions\n",
    "- **Transformer**: Better at capturing global dependencies through self-attention\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Input (batch, 32768, 2)\n",
    "    |\n",
    "    v\n",
    "DualChannelStem (Conv1D feature extraction)\n",
    "    |\n",
    "    v\n",
    "Multi-Resolution TCN (local patterns, d=1,2,4,8,16,32)\n",
    "    |\n",
    "    v\n",
    "Temporal Downsampling (32768 -> 512 for memory efficiency)\n",
    "    |\n",
    "    v\n",
    "Transformer Encoder (global context, 4 layers)\n",
    "    |\n",
    "    v\n",
    "RUL Head\n",
    "```\n",
    "\n",
    "### Key Differences from Pattern 1\n",
    "- More Transformer layers (4 vs 2)\n",
    "- Deeper TCN stack\n",
    "- No cross-attention (simpler pipeline)\n",
    "- Higher downsampling factor for longer Transformer context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TCNTransformerHybridConfig:\n",
    "    \"\"\"Configuration for TCN -> Transformer hybrid model.\"\"\"\n",
    "    input_length: int = 32768\n",
    "    num_channels: int = 2\n",
    "    \n",
    "    # Stem config\n",
    "    stem_filters: int = 64\n",
    "    \n",
    "    # TCN config (local feature extraction)\n",
    "    tcn_filters: int = 64\n",
    "    tcn_dilations: list = field(default_factory=lambda: [1, 2, 4, 8, 16, 32])\n",
    "    tcn_kernel_size: int = 3\n",
    "    tcn_dropout: float = 0.1\n",
    "    \n",
    "    # Downsampling\n",
    "    downsample_factor: int = 64  # Higher factor for longer Transformer context\n",
    "    \n",
    "    # Transformer config (global context)\n",
    "    transformer_layers: int = 4  # More layers than Pattern 1\n",
    "    transformer_heads: int = 4\n",
    "    transformer_key_dim: int = 64\n",
    "    transformer_ff_dim: int = 128\n",
    "    transformer_dropout: float = 0.1\n",
    "    \n",
    "    # RUL head\n",
    "    hidden_dim: int = 64\n",
    "    dropout_rate: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tcn_transformer_hybrid(\n",
    "    config: Optional[TCNTransformerHybridConfig] = None,\n",
    "    name: str = 'tcn_transformer_hybrid'\n",
    ") -> keras.Model:\n",
    "    \"\"\"Build TCN -> Transformer hybrid model.\n",
    "    \n",
    "    This architecture uses TCN for local feature extraction\n",
    "    followed by Transformer for global temporal modeling.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = TCNTransformerHybridConfig()\n",
    "    \n",
    "    # Input\n",
    "    inputs = keras.Input(\n",
    "        shape=(config.input_length, config.num_channels),\n",
    "        name='input'\n",
    "    )\n",
    "    \n",
    "    # Per-channel stem\n",
    "    stem = DualChannelStem(\n",
    "        config=StemConfig(filters=config.stem_filters),\n",
    "        share_weights=False,\n",
    "        name='stem'\n",
    "    )\n",
    "    h_stem, v_stem = stem(inputs)\n",
    "    \n",
    "    # TCN encoding (local patterns)\n",
    "    tcn = DualChannelTCN(\n",
    "        config=TCNConfig(\n",
    "            filters=config.tcn_filters,\n",
    "            kernel_size=config.tcn_kernel_size,\n",
    "            dilations=config.tcn_dilations,\n",
    "            dropout_rate=config.tcn_dropout,\n",
    "        ),\n",
    "        share_weights=False,\n",
    "        name='tcn'\n",
    "    )\n",
    "    h_tcn, v_tcn = tcn((h_stem, v_stem))\n",
    "    \n",
    "    # Concatenate channels before downsampling\n",
    "    concat = layers.Concatenate(axis=-1, name='channel_concat')([h_tcn, v_tcn])\n",
    "    \n",
    "    # Temporal downsampling (for memory-efficient Transformer)\n",
    "    downsampler = TemporalDownsampler(\n",
    "        factor=config.downsample_factor,\n",
    "        mode='avg',\n",
    "        name='downsample'\n",
    "    )\n",
    "    downsampled = downsampler(concat)\n",
    "    \n",
    "    # Transformer encoder (global context)\n",
    "    transformer = TransformerAggregator(\n",
    "        config=TransformerAggregatorConfig(\n",
    "            num_layers=config.transformer_layers,\n",
    "            num_heads=config.transformer_heads,\n",
    "            key_dim=config.transformer_key_dim,\n",
    "            ff_dim=config.transformer_ff_dim,\n",
    "            dropout_rate=config.transformer_dropout,\n",
    "            use_cls_token=True,\n",
    "            pooling='cls',\n",
    "        ),\n",
    "        name='transformer'\n",
    "    )\n",
    "    aggregated = transformer(downsampled)\n",
    "    \n",
    "    # RUL head\n",
    "    rul_head = RULHead(\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        name='rul_head'\n",
    "    )\n",
    "    output = rul_head(aggregated)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=output, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TCN-Transformer hybrid\n",
    "config_tcn_transformer = TCNTransformerHybridConfig(\n",
    "    stem_filters=64,\n",
    "    tcn_filters=64,\n",
    "    tcn_dilations=[1, 2, 4, 8, 16, 32],\n",
    "    downsample_factor=64,\n",
    "    transformer_layers=4,\n",
    "    transformer_heads=4,\n",
    ")\n",
    "\n",
    "model_tcn_transformer = build_tcn_transformer_hybrid(config_tcn_transformer)\n",
    "\n",
    "print('=== TCN -> Transformer Hybrid ===')\n",
    "print_model_summary_p1(model_tcn_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model summary\n",
    "model_tcn_transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hybrid 2: 2D CNN + State Space Model (SSM)\n",
    "\n",
    "State Space Models (like S4, Mamba) are an alternative to Transformers for sequence modeling:\n",
    "- **Linear time complexity** O(n) vs O(n²) for Transformers\n",
    "- **Better at very long sequences** due to continuous-time formulation\n",
    "- **Recurrent inference** makes them efficient at test time\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Input Spectrograms (batch, 128, 128, 2)\n",
    "    |\n",
    "    v\n",
    "DualChannel CNN Backbone (spatial features)\n",
    "    |\n",
    "    v\n",
    "Late Fusion (concat H, V embeddings)\n",
    "    |\n",
    "    v\n",
    "State Space Layer (temporal dynamics)\n",
    "    |\n",
    "    v\n",
    "RUL Head\n",
    "```\n",
    "\n",
    "### Simplified SSM Implementation\n",
    "\n",
    "We implement a simplified S4-inspired layer using:\n",
    "- Learnable state transition matrix A\n",
    "- Discretization via bilinear transform\n",
    "- Parallel scan for efficient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSSMLayer(keras.layers.Layer):\n",
    "    \"\"\"Simplified State Space Model layer.\n",
    "    \n",
    "    Implements a basic linear state space model:\n",
    "        h_t = A * h_{t-1} + B * x_t\n",
    "        y_t = C * h_t + D * x_t\n",
    "    \n",
    "    Uses diagonal state matrix for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int = 64,\n",
    "        output_dim: int = 64,\n",
    "        dropout_rate: float = 0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_dim = state_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.input_proj = None\n",
    "        self.state_proj = None\n",
    "        self.output_proj = None\n",
    "        self.dropout = None\n",
    "        self.layer_norm = None\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        \n",
    "        # B: Input to state projection\n",
    "        self.input_proj = layers.Dense(\n",
    "            self.state_dim,\n",
    "            use_bias=False,\n",
    "            name=f'{self.name}_B'\n",
    "        )\n",
    "        \n",
    "        # A: State transition (diagonal, initialized near identity for stability)\n",
    "        # Use sigmoid to keep values in (0, 1) for stability\n",
    "        self.A_logit = self.add_weight(\n",
    "            name='A_logit',\n",
    "            shape=(self.state_dim,),\n",
    "            initializer=keras.initializers.Constant(2.0),  # sigmoid(2) ~ 0.88\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # C: State to output projection\n",
    "        self.state_proj = layers.Dense(\n",
    "            self.output_dim,\n",
    "            use_bias=False,\n",
    "            name=f'{self.name}_C'\n",
    "        )\n",
    "        \n",
    "        # D: Skip connection\n",
    "        self.output_proj = layers.Dense(\n",
    "            self.output_dim,\n",
    "            name=f'{self.name}_D'\n",
    "        )\n",
    "        \n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.layer_norm = layers.LayerNormalization(name=f'{self.name}_ln')\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Process sequence through SSM.\n",
    "        \n",
    "        Args:\n",
    "            inputs: (batch, seq_len, features)\n",
    "            \n",
    "        Returns:\n",
    "            (batch, output_dim) - aggregated output\n",
    "        \"\"\"\n",
    "        # A in (0, 1) for stability\n",
    "        A = keras.ops.sigmoid(self.A_logit)\n",
    "        \n",
    "        # Project input\n",
    "        Bx = self.input_proj(inputs)  # (batch, seq, state_dim)\n",
    "        \n",
    "        # Recurrent scan (simplified parallel implementation)\n",
    "        # For efficiency, we use a simple loop here\n",
    "        # In practice, you'd use parallel scan for GPU efficiency\n",
    "        batch_size = keras.ops.shape(inputs)[0]\n",
    "        seq_len = keras.ops.shape(inputs)[1]\n",
    "        \n",
    "        # Initialize state\n",
    "        h = keras.ops.zeros((batch_size, self.state_dim))\n",
    "        outputs = []\n",
    "        \n",
    "        # Simple unrolled scan for short sequences\n",
    "        # Note: For production, use tf.scan or parallel associative scan\n",
    "        for t in range(keras.ops.convert_to_numpy(seq_len)):\n",
    "            h = A * h + Bx[:, t, :]\n",
    "            outputs.append(h)\n",
    "        \n",
    "        # Stack outputs\n",
    "        all_states = keras.ops.stack(outputs, axis=1)  # (batch, seq, state_dim)\n",
    "        \n",
    "        # Project to output and add skip connection\n",
    "        y = self.state_proj(all_states) + self.output_proj(inputs)\n",
    "        \n",
    "        # Layer norm and dropout\n",
    "        y = self.layer_norm(y)\n",
    "        y = self.dropout(y, training=training)\n",
    "        \n",
    "        # Return last state as summary (or could use mean pooling)\n",
    "        return y[:, -1, :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'state_dim': self.state_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Bidirectional SSM with GRU-like gating\n",
    "class GatedSSMLayer(keras.layers.Layer):\n",
    "    \"\"\"Gated State Space Model layer with bidirectional processing.\n",
    "    \n",
    "    Combines SSM dynamics with GRU-style gating for better gradient flow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int = 64,\n",
    "        bidirectional: bool = True,\n",
    "        dropout_rate: float = 0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        \n",
    "        # Forward GRU-like layer (approximates SSM with gating)\n",
    "        self.gru_fwd = layers.GRU(\n",
    "            self.units,\n",
    "            return_sequences=True,\n",
    "            name=f'{self.name}_gru_fwd'\n",
    "        )\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.gru_bwd = layers.GRU(\n",
    "                self.units,\n",
    "                return_sequences=True,\n",
    "                go_backwards=True,\n",
    "                name=f'{self.name}_gru_bwd'\n",
    "            )\n",
    "        \n",
    "        # Linear mixing (SSM-inspired)\n",
    "        out_dim = self.units * 2 if self.bidirectional else self.units\n",
    "        self.mix = layers.Dense(self.units, name=f'{self.name}_mix')\n",
    "        \n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.layer_norm = layers.LayerNormalization(name=f'{self.name}_ln')\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        # Forward pass\n",
    "        fwd = self.gru_fwd(inputs)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Backward pass\n",
    "            bwd = self.gru_bwd(inputs)\n",
    "            # Reverse to align with forward\n",
    "            bwd = keras.ops.flip(bwd, axis=1)\n",
    "            # Concatenate\n",
    "            combined = keras.ops.concatenate([fwd, bwd], axis=-1)\n",
    "        else:\n",
    "            combined = fwd\n",
    "        \n",
    "        # Linear mixing\n",
    "        mixed = self.mix(combined)\n",
    "        \n",
    "        # Norm and dropout\n",
    "        mixed = self.layer_norm(mixed)\n",
    "        mixed = self.dropout(mixed, training=training)\n",
    "        \n",
    "        # Return last timestep\n",
    "        return mixed[:, -1, :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'bidirectional': self.bidirectional,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CNNSSMHybridConfig:\n",
    "    \"\"\"Configuration for 2D CNN + SSM hybrid model.\"\"\"\n",
    "    # Input (spectrograms)\n",
    "    spectrogram_height: int = 128\n",
    "    spectrogram_width: int = 128\n",
    "    \n",
    "    # CNN backbone\n",
    "    cnn_filters: list = field(default_factory=lambda: [32, 64, 128, 256])\n",
    "    cnn_kernel_sizes: list = field(default_factory=lambda: [3, 3, 3, 3])\n",
    "    share_weights: bool = True\n",
    "    \n",
    "    # Fusion\n",
    "    fusion_mode: str = 'concat'\n",
    "    \n",
    "    # SSM config\n",
    "    ssm_type: str = 'gated'  # 'simple' or 'gated'\n",
    "    ssm_units: int = 64\n",
    "    ssm_bidirectional: bool = True\n",
    "    ssm_dropout: float = 0.1\n",
    "    \n",
    "    # RUL head\n",
    "    hidden_dim: int = 64\n",
    "    dropout_rate: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_ssm_hybrid(\n",
    "    config: Optional[CNNSSMHybridConfig] = None,\n",
    "    name: str = 'cnn_ssm_hybrid'\n",
    ") -> keras.Model:\n",
    "    \"\"\"Build 2D CNN + State Space Model hybrid.\n",
    "    \n",
    "    Uses CNN for spatial feature extraction from spectrograms,\n",
    "    followed by SSM for temporal modeling.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = CNNSSMHybridConfig()\n",
    "    \n",
    "    # Input\n",
    "    inputs = keras.Input(\n",
    "        shape=(config.spectrogram_height, config.spectrogram_width, 2),\n",
    "        name='spectrogram_input'\n",
    "    )\n",
    "    \n",
    "    # 2D CNN backbone\n",
    "    backbone = DualChannelCNN2DBackbone(\n",
    "        config=CNN2DBackboneConfig(\n",
    "            filters=config.cnn_filters,\n",
    "            kernel_sizes=config.cnn_kernel_sizes,\n",
    "        ),\n",
    "        share_weights=config.share_weights,\n",
    "        name='backbone'\n",
    "    )\n",
    "    h_emb, v_emb = backbone(inputs)\n",
    "    \n",
    "    # Late fusion\n",
    "    fusion = LateFusion(\n",
    "        fusion_mode=config.fusion_mode,\n",
    "        name='fusion'\n",
    "    )\n",
    "    fused = fusion((h_emb, v_emb))\n",
    "    \n",
    "    # Expand to sequence for SSM (single spectrogram = length 1)\n",
    "    # In practice, you might have multiple spectrograms per bearing\n",
    "    fused_seq = layers.Reshape((1, -1), name='expand_seq')(fused)\n",
    "    \n",
    "    # State Space Model\n",
    "    if config.ssm_type == 'simple':\n",
    "        ssm = SimpleSSMLayer(\n",
    "            state_dim=config.ssm_units,\n",
    "            output_dim=config.ssm_units,\n",
    "            dropout_rate=config.ssm_dropout,\n",
    "            name='ssm'\n",
    "        )\n",
    "    else:  # gated\n",
    "        ssm = GatedSSMLayer(\n",
    "            units=config.ssm_units,\n",
    "            bidirectional=config.ssm_bidirectional,\n",
    "            dropout_rate=config.ssm_dropout,\n",
    "            name='ssm'\n",
    "        )\n",
    "    aggregated = ssm(fused_seq)\n",
    "    \n",
    "    # RUL head\n",
    "    x = layers.Dense(config.hidden_dim, activation='gelu', name='rul_hidden')(aggregated)\n",
    "    x = layers.Dropout(config.dropout_rate, name='rul_dropout')(x)\n",
    "    output = layers.Dense(1, activation='relu', name='rul_output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=output, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN-SSM hybrid\n",
    "config_cnn_ssm = CNNSSMHybridConfig(\n",
    "    cnn_filters=[32, 64, 128, 256],\n",
    "    ssm_type='gated',\n",
    "    ssm_units=64,\n",
    "    ssm_bidirectional=True,\n",
    ")\n",
    "\n",
    "model_cnn_ssm = build_cnn_ssm_hybrid(config_cnn_ssm)\n",
    "\n",
    "print('=== 2D CNN + SSM Hybrid ===')\n",
    "print_model_summary_p2(model_cnn_ssm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model summary\n",
    "model_cnn_ssm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Data for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bearing data\n",
    "loader = XJTUBearingLoader()\n",
    "\n",
    "# Load a single bearing for experiments\n",
    "condition = '35Hz12kN'\n",
    "bearing_id = 'Bearing1_1'\n",
    "\n",
    "print(f'Loading {bearing_id} from {condition}...')\n",
    "signals, file_paths = loader.load_bearing(condition, bearing_id)\n",
    "\n",
    "# Generate RUL labels\n",
    "rul_labels = generate_rul_labels(\n",
    "    num_files=len(file_paths),\n",
    "    strategy='piecewise_linear',\n",
    "    max_rul=125\n",
    ")\n",
    "\n",
    "print(f'Signal shape: {signals.shape}')  # (num_files, 32768, 2)\n",
    "print(f'RUL labels shape: {rul_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spectrograms for CNN-SSM model\n",
    "print('Generating spectrograms...')\n",
    "\n",
    "n_samples = min(50, len(signals))  # Use subset for speed\n",
    "spectrograms = np.array([\n",
    "    extract_spectrogram(signals[i])\n",
    "    for i in range(n_samples)\n",
    "])\n",
    "\n",
    "print(f'Spectrogram shape: {spectrograms.shape}')  # (n, 128, 128, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for both models\n",
    "X_raw = signals[:n_samples]\n",
    "X_spec = spectrograms\n",
    "y = rul_labels[:n_samples].reshape(-1, 1)\n",
    "\n",
    "# Train/val split\n",
    "split_idx = int(0.8 * n_samples)\n",
    "\n",
    "X_raw_train, X_raw_val = X_raw[:split_idx], X_raw[split_idx:]\n",
    "X_spec_train, X_spec_val = X_spec[:split_idx], X_spec[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f'Training samples: {len(y_train)}')\n",
    "print(f'Validation samples: {len(y_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=4,\n",
    "    epochs=10,  # Quick demo\n",
    ")\n",
    "\n",
    "def train_and_evaluate(model, X_train, X_val, y_train, y_val, name):\n",
    "    \"\"\"Train model and return metrics.\"\"\"\n",
    "    # Compile\n",
    "    compile_model(model, training_config)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=training_config.epochs,\n",
    "        batch_size=training_config.batch_size,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(np.vstack([X_train, X_val]), verbose=0).flatten()\n",
    "    y_true = np.vstack([y_train, y_val]).flatten()\n",
    "    \n",
    "    metrics = {\n",
    "        'model': name,\n",
    "        'rmse': rmse(y_true, y_pred),\n",
    "        'mae': mae(y_true, y_pred),\n",
    "        'phm08': phm08_score(y_true, y_pred),\n",
    "        'train_loss': history.history['loss'][-1],\n",
    "        'val_loss': history.history['val_loss'][-1],\n",
    "    }\n",
    "    \n",
    "    return history, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TCN-Transformer Hybrid\n",
    "print('=' * 50)\n",
    "print('Training TCN-Transformer Hybrid...')\n",
    "print('=' * 50)\n",
    "\n",
    "# Use smaller model for demo\n",
    "config_small = TCNTransformerHybridConfig(\n",
    "    stem_filters=32,\n",
    "    tcn_filters=32,\n",
    "    tcn_dilations=[1, 2, 4, 8],\n",
    "    downsample_factor=128,  # Higher downsampling\n",
    "    transformer_layers=2,\n",
    "    transformer_heads=2,\n",
    "    transformer_key_dim=32,\n",
    "    transformer_ff_dim=64,\n",
    ")\n",
    "model_tcn_transformer_small = build_tcn_transformer_hybrid(config_small, 'tcn_transformer_small')\n",
    "\n",
    "history_tcn, metrics_tcn = train_and_evaluate(\n",
    "    model_tcn_transformer_small,\n",
    "    X_raw_train, X_raw_val,\n",
    "    y_train, y_val,\n",
    "    'TCN-Transformer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN-SSM Hybrid\n",
    "print('=' * 50)\n",
    "print('Training CNN-SSM Hybrid...')\n",
    "print('=' * 50)\n",
    "\n",
    "# Use smaller model for demo\n",
    "config_small_ssm = CNNSSMHybridConfig(\n",
    "    cnn_filters=[16, 32, 64, 128],\n",
    "    ssm_type='gated',\n",
    "    ssm_units=32,\n",
    ")\n",
    "model_cnn_ssm_small = build_cnn_ssm_hybrid(config_small_ssm, 'cnn_ssm_small')\n",
    "\n",
    "history_ssm, metrics_ssm = train_and_evaluate(\n",
    "    model_cnn_ssm_small,\n",
    "    X_spec_train, X_spec_val,\n",
    "    y_train, y_val,\n",
    "    'CNN-SSM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare Architectures on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "results = pd.DataFrame([metrics_tcn, metrics_ssm])\n",
    "print('\\n=== Architecture Comparison ===')\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history_tcn.history['loss'], 'b-', label='TCN-Transformer Train', linewidth=2)\n",
    "ax1.plot(history_tcn.history['val_loss'], 'b--', label='TCN-Transformer Val', linewidth=2)\n",
    "ax1.plot(history_ssm.history['loss'], 'r-', label='CNN-SSM Train', linewidth=2)\n",
    "ax1.plot(history_ssm.history['val_loss'], 'r--', label='CNN-SSM Val', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Huber Loss')\n",
    "ax1.set_title('Training Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics bar chart\n",
    "ax2 = axes[1]\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "metrics_names = ['RMSE', 'MAE', 'Val Loss']\n",
    "tcn_values = [metrics_tcn['rmse'], metrics_tcn['mae'], metrics_tcn['val_loss']]\n",
    "ssm_values = [metrics_ssm['rmse'], metrics_ssm['mae'], metrics_ssm['val_loss']]\n",
    "\n",
    "ax2.bar(x - width/2, tcn_values, width, label='TCN-Transformer', color='steelblue')\n",
    "ax2.bar(x + width/2, ssm_values, width, label='CNN-SSM', color='coral')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Metrics Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics_names)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction comparison\n",
    "y_pred_tcn = model_tcn_transformer_small.predict(X_raw, verbose=0).flatten()\n",
    "y_pred_ssm = model_cnn_ssm_small.predict(X_spec, verbose=0).flatten()\n",
    "y_true = y.flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series\n",
    "ax1 = axes[0]\n",
    "ax1.plot(y_true, 'k-', label='Ground Truth', linewidth=2)\n",
    "ax1.plot(y_pred_tcn, 'b--', label='TCN-Transformer', linewidth=2, alpha=0.7)\n",
    "ax1.plot(y_pred_ssm, 'r--', label='CNN-SSM', linewidth=2, alpha=0.7)\n",
    "ax1.set_xlabel('Sample Index')\n",
    "ax1.set_ylabel('RUL')\n",
    "ax1.set_title('RUL Predictions Over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_true, y_pred_tcn, alpha=0.6, label='TCN-Transformer', color='steelblue')\n",
    "ax2.scatter(y_true, y_pred_ssm, alpha=0.6, label='CNN-SSM', color='coral')\n",
    "max_val = max(y_true.max(), y_pred_tcn.max(), y_pred_ssm.max())\n",
    "ax2.plot([0, max_val], [0, max_val], 'k--', label='Perfect')\n",
    "ax2.set_xlabel('True RUL')\n",
    "ax2.set_ylabel('Predicted RUL')\n",
    "ax2.set_title('Prediction Scatter Plot')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Document Findings and Architecture Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model statistics\n",
    "print('=== Model Statistics ===')\n",
    "print('\\nTCN-Transformer Hybrid:')\n",
    "print_model_summary_p1(model_tcn_transformer_small)\n",
    "\n",
    "print('\\nCNN-SSM Hybrid:')\n",
    "print_model_summary_p2(model_cnn_ssm_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "\n",
    "### TCN-Transformer Hybrid\n",
    "\n",
    "**Pros:**\n",
    "- Works directly on raw signals (no preprocessing)\n",
    "- TCN provides efficient multi-scale local feature extraction\n",
    "- Transformer captures global dependencies\n",
    "- Dilated convolutions give large receptive field with fewer parameters\n",
    "\n",
    "**Cons:**\n",
    "- Requires significant downsampling before Transformer (memory constraints)\n",
    "- Longer training time due to sequence length\n",
    "- May lose fine-grained temporal information after downsampling\n",
    "\n",
    "**Best for:**\n",
    "- When raw signal access is important\n",
    "- When multi-scale patterns matter\n",
    "- When computational resources are available\n",
    "\n",
    "---\n",
    "\n",
    "### CNN-SSM Hybrid\n",
    "\n",
    "**Pros:**\n",
    "- CNN is highly efficient for spectrogram processing\n",
    "- SSM has linear time complexity O(n)\n",
    "- More memory efficient than Transformer-based models\n",
    "- Spectrograms provide time-frequency representation\n",
    "\n",
    "**Cons:**\n",
    "- Requires spectrogram preprocessing\n",
    "- Information loss during spectrogram generation\n",
    "- Simplified SSM may not capture all temporal dynamics\n",
    "\n",
    "**Best for:**\n",
    "- When memory is limited\n",
    "- When spectrograms are pre-computed\n",
    "- When linear scaling with sequence length is needed\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **For production deployment**: CNN-SSM hybrid is more efficient and scalable\n",
    "2. **For maximum accuracy**: TCN-Transformer hybrid may capture more nuanced patterns\n",
    "3. **For ensemble**: Combine both models for potentially better predictions\n",
    "4. **Future work**: \n",
    "   - Implement proper S4/Mamba SSM for better temporal modeling\n",
    "   - Explore attention mechanisms in SSM (like Mamba's selective state spaces)\n",
    "   - Test with multi-spectrogram sequences for longer context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Aspect | TCN-Transformer Hybrid | CNN-SSM Hybrid |\n",
    "|--------|----------------------|----------------|\n",
    "| Input | Raw signals (32768, 2) | Spectrograms (128, 128, 2) |\n",
    "| Feature Extractor | TCN (dilated convolutions) | 2D CNN (spatial) |\n",
    "| Temporal Model | Transformer Encoder | State Space Model |\n",
    "| Time Complexity | O(n²) in attention | O(n) linear |\n",
    "| Memory | High (needs downsampling) | Lower |\n",
    "| Preprocessing | None | STFT required |\n",
    "| Interpretability | Attention weights | State dynamics |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
