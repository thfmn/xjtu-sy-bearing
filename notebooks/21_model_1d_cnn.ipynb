{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN-Transformer Model Training (MODEL-4)\n",
    "\n",
    "This notebook demonstrates training the TCN-Transformer Pattern 1 architecture for RUL prediction.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Input (batch, 32768, 2)  # Raw vibration signals (H, V channels)\n",
    "    ↓\n",
    "DualChannelStem          # Per-sensor Conv1D(k=7) → GELU → Conv1D(k=3)\n",
    "    ↓\n",
    "DualChannelTCN           # Dilated convolutions (d=1,2,4,8,16,32)\n",
    "    ↓\n",
    "TemporalDownsample       # Reduce sequence length (32768 → 2048)\n",
    "    ↓\n",
    "BidirectionalCrossAttn   # H attends to V, V attends to H\n",
    "    ↓\n",
    "ChannelFusion            # Concatenate H and V features\n",
    "    ↓\n",
    "TemporalAggregator       # LSTM (v1) or Transformer (v2)\n",
    "    ↓\n",
    "RULHead                  # Dense → ReLU → RUL prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** This notebook documents exploratory model development. ",
    "The architectures investigated here informed the final benchmark models ",
    "but are not part of the production evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pattern1 import (\n",
    "    create_tcn_transformer_lstm,\n",
    "    create_tcn_transformer_transformer,\n",
    "    TCNTransformerConfig,\n",
    "    StemConfig,\n",
    "    TCNConfig,\n",
    "    AttentionConfig,\n",
    "    LSTMAggregatorConfig,\n",
    "    build_tcn_transformer_model,\n",
    "    print_model_summary,\n",
    ")\n",
    "\n",
    "from src.training.config import TrainingConfig, compile_model, build_callbacks\n",
    "from src.training.metrics import rmse, mae, phm08_score, print_evaluation_report\n",
    "from src.training.cv import leave_one_bearing_out_cv, CVSplit\n",
    "\n",
    "from src.data.loader import XJTUBearingLoader\n",
    "from src.data.rul_labels import generate_rul_labels\n",
    "\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Inspect Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM aggregator variant (v1)\n",
    "model_lstm = create_tcn_transformer_lstm(\n",
    "    input_length=32768,\n",
    "    filters=64,\n",
    "    dilations=[1, 2, 4, 8, 16, 32],\n",
    "    lstm_units=64,\n",
    ")\n",
    "\n",
    "print('=== TCN-Transformer with LSTM Aggregator (v1) ===')\n",
    "print_model_summary(model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer aggregator variant (v2)\n",
    "model_transformer = create_tcn_transformer_transformer(\n",
    "    input_length=32768,\n",
    "    filters=64,\n",
    "    dilations=[1, 2, 4, 8, 16, 32],\n",
    "    num_transformer_layers=2,\n",
    "    num_heads=4,\n",
    ")\n",
    "\n",
    "print('=== TCN-Transformer with Transformer Aggregator (v2) ===')\n",
    "print_model_summary(model_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full model architecture\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TCN Receptive Field Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pattern1.tcn import TCNEncoder, TCNConfig\n",
    "\n",
    "# Analyze receptive field\n",
    "tcn_config = TCNConfig(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    dilations=[1, 2, 4, 8, 16, 32],\n",
    ")\n",
    "\n",
    "tcn = TCNEncoder(config=tcn_config)\n",
    "tcn.build((None, 32768, 64))\n",
    "\n",
    "rf = tcn.compute_receptive_field()\n",
    "sampling_rate = 25600  # Hz\n",
    "rf_ms = rf / sampling_rate * 1000\n",
    "\n",
    "print(f'TCN Configuration:')\n",
    "print(f'  Kernel size: {tcn_config.kernel_size}')\n",
    "print(f'  Dilations: {tcn_config.dilations}')\n",
    "print(f'  Receptive field: {rf} samples ({rf_ms:.2f} ms at 25.6kHz)')\n",
    "print(f'  Input coverage: {rf/32768*100:.2f}% of full signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bearing data\n",
    "loader = XJTUBearingLoader()\n",
    "metadata = loader.get_metadata()\n",
    "\n",
    "print(f'Dataset: {metadata[\"total_files\"]} files')\n",
    "print(f'Conditions: {list(metadata[\"conditions\"].keys())}')\n",
    "print(f'Bearings per condition: 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single bearing for demonstration\n",
    "condition = '35Hz12kN'\n",
    "bearing_id = 'Bearing1_1'\n",
    "\n",
    "print(f'Loading {bearing_id} from {condition}...')\n",
    "signals, file_paths = loader.load_bearing(condition, bearing_id)\n",
    "\n",
    "# Generate RUL labels\n",
    "rul_labels = generate_rul_labels(\n",
    "    num_files=len(file_paths),\n",
    "    strategy='piecewise_linear',\n",
    "    max_rul=125\n",
    ")\n",
    "\n",
    "print(f'Signal shape: {signals.shape}')  # (num_files, 32768, 2)\n",
    "print(f'RUL labels shape: {rul_labels.shape}')\n",
    "print(f'RUL range: [{rul_labels.min():.1f}, {rul_labels.max():.1f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Training Demo\n",
    "\n",
    "Demonstrate training on a subset of data to verify the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subset for demo (first 50 samples)\n",
    "n_demo = min(50, len(signals))\n",
    "X_demo = signals[:n_demo]\n",
    "y_demo = rul_labels[:n_demo].reshape(-1, 1)\n",
    "\n",
    "# Train/val split\n",
    "split_idx = int(0.8 * n_demo)\n",
    "X_train, X_val = X_demo[:split_idx], X_demo[split_idx:]\n",
    "y_train, y_val = y_demo[:split_idx], y_demo[split_idx:]\n",
    "\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Validation samples: {len(X_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = create_tcn_transformer_lstm(input_length=32768, filters=32)  # Smaller for demo\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=4,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "compile_model(model, training_config)\n",
    "print('Model compiled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=training_config.epochs,\n",
    "    batch_size=training_config.batch_size,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(history.history['loss'], label='Train Loss')\n",
    "ax.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Huber Loss')\n",
    "ax.set_title('Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_demo, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = y_demo.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "print('=== Evaluation Metrics ===')\n",
    "print(f'RMSE: {rmse(y_true, y_pred_flat):.2f}')\n",
    "print(f'MAE: {mae(y_true, y_pred_flat):.2f}')\n",
    "print(f'PHM08 Score: {phm08_score(y_true, y_pred_flat):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs ground truth\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series plot\n",
    "ax1 = axes[0]\n",
    "ax1.plot(y_true, 'b-', label='Ground Truth', linewidth=2)\n",
    "ax1.plot(y_pred_flat, 'r--', label='Prediction', linewidth=2)\n",
    "ax1.set_xlabel('Sample Index')\n",
    "ax1.set_ylabel('RUL')\n",
    "ax1.set_title('RUL Prediction vs Ground Truth')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_true, y_pred_flat, alpha=0.6)\n",
    "max_val = max(y_true.max(), y_pred_flat.max())\n",
    "ax2.plot([0, max_val], [0, max_val], 'k--', label='Perfect Prediction')\n",
    "ax2.set_xlabel('True RUL')\n",
    "ax2.set_ylabel('Predicted RUL')\n",
    "ax2.set_title('Prediction Scatter Plot')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Configuration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create custom configuration\n",
    "custom_config = TCNTransformerConfig(\n",
    "    input_length=32768,\n",
    "    num_channels=2,\n",
    "    stem_config=StemConfig(\n",
    "        filters=32,\n",
    "        kernel_size_1=7,\n",
    "        kernel_size_2=3,\n",
    "        use_batch_norm=True,\n",
    "    ),\n",
    "    tcn_config=TCNConfig(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        dilations=[1, 2, 4, 8, 16],  # Shorter dilations\n",
    "        use_batch_norm=True,\n",
    "        dropout_rate=0.2,\n",
    "    ),\n",
    "    attention_config=AttentionConfig(\n",
    "        num_heads=2,\n",
    "        key_dim=32,\n",
    "        dropout_rate=0.1,\n",
    "    ),\n",
    "    aggregator_type='lstm',\n",
    "    lstm_config=LSTMAggregatorConfig(\n",
    "        units=32,\n",
    "        bidirectional=True,\n",
    "        pooling='last',\n",
    "    ),\n",
    "    fusion_mode='concat',\n",
    "    hidden_dim=32,\n",
    "    dropout_rate=0.1,\n",
    "    use_downsampling=True,\n",
    "    downsample_factor=16,\n",
    ")\n",
    "\n",
    "custom_model = build_tcn_transformer_model(custom_config, name='custom_tcn_transformer')\n",
    "print_model_summary(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison Summary\n",
    "\n",
    "| Model | Parameters | Aggregator | Use Case |\n",
    "|-------|------------|------------|----------|\n",
    "| TCN-Transformer-LSTM (v1) | ~418K | Bidirectional LSTM | General purpose, good baseline |\n",
    "| TCN-Transformer-Transformer (v2) | ~1.2M | Transformer Encoder | Better for long-range dependencies |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Architecture Highlights\n",
    "- **Per-sensor stem**: Learns channel-specific features with Conv1D(k=7) → GELU → Conv1D(k=3)\n",
    "- **Multi-resolution TCN**: Dilated convolutions capture patterns at multiple time scales\n",
    "- **Cross-attention**: Allows H and V channels to exchange information\n",
    "- **Temporal downsampling**: Reduces sequence length before attention to manage memory\n",
    "- **Monotonic RUL output**: ReLU ensures non-negative predictions\n",
    "\n",
    "### Training Tips\n",
    "1. Start with smaller `filters` (32) for faster iteration\n",
    "2. Use `use_downsampling=True` with `downsample_factor=16` to manage memory\n",
    "3. Monitor validation loss to detect overfitting\n",
    "4. Consider windowing for very large datasets (see `src/data/dataset.py`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
