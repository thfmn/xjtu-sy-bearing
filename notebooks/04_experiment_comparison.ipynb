{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Comparison\n",
    "\n",
    "This notebook demonstrates how to query and compare MLflow experiment runs\n",
    "from the XJTU-SY bearing RUL training pipeline.\n",
    "\n",
    "**Prerequisites**: Run at least one training fold via\n",
    "`python scripts/05_train_dl_models.py --model <model_name> --folds 0`\n",
    "so that MLflow runs exist in `mlruns/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Point to local MLflow tracking store\n",
    "mlflow.set_tracking_uri(\"../mlruns\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: List all experiments\n",
    "experiments = mlflow.search_experiments()\n",
    "print(f\"Found {len(experiments)} experiment(s):\\n\")\n",
    "for exp in experiments:\n",
    "    print(f\"  ID: {exp.experiment_id}  Name: {exp.name}  Artifact location: {exp.artifact_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: List all runs for the bearing_rul_dl experiment\n",
    "EXPERIMENT_NAME = \"bearing_rul_dl\"\n",
    "\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_names=[EXPERIMENT_NAME],\n",
    "    order_by=[\"start_time DESC\"],\n",
    ")\n",
    "\n",
    "if runs_df.empty:\n",
    "    print(f\"No runs found for experiment '{EXPERIMENT_NAME}'.\")\n",
    "    print(\"Run training first: python scripts/05_train_dl_models.py --model cnn1d_baseline --folds 0\")\n",
    "else:\n",
    "    print(f\"Found {len(runs_df)} run(s):\\n\")\n",
    "    # Show key columns\n",
    "    display_cols = [\"run_id\", \"status\", \"start_time\"]\n",
    "    param_cols = [c for c in runs_df.columns if c.startswith(\"params.\")]\n",
    "    metric_cols = [c for c in runs_df.columns if c.startswith(\"metrics.\")]\n",
    "    display_cols += param_cols[:6] + metric_cols[:6]\n",
    "    display(runs_df[[c for c in display_cols if c in runs_df.columns]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Compare runs across folds using the project's compare_runs() utility\n",
    "from src.utils.tracking import ExperimentTracker, compare_runs\n",
    "\n",
    "tracker = ExperimentTracker(\n",
    "    backend=\"mlflow\",\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    tracking_uri=\"../mlruns\",\n",
    ")\n",
    "\n",
    "runs = tracker.list_runs(experiment_name=EXPERIMENT_NAME)\n",
    "print(f\"Retrieved {len(runs)} RunInfo objects\")\n",
    "\n",
    "if runs:\n",
    "    comparison = compare_runs(\n",
    "        runs,\n",
    "        metrics=[\"final_rmse\", \"final_mae\", \"final_phm08_score\"],\n",
    "        params=[\"model_name\", \"fold_id\"],\n",
    "    )\n",
    "    print(\"\\n--- Metric Statistics ---\")\n",
    "    for metric_name, stats in comparison[\"metric_stats\"].items():\n",
    "        print(f\"  {metric_name}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, \"\n",
    "              f\"min={stats['min']:.2f}, max={stats['max']:.2f}\")\n",
    "\n",
    "    if comparison[\"best_run\"]:\n",
    "        best = comparison[\"best_run\"]\n",
    "        print(f\"\\nBest run: {best.run_name} (ID: {best.run_id[:8]}...)\")\n",
    "\n",
    "    # Show summary table\n",
    "    summary_df = comparison[\"dataframe\"]\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"No runs available for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Plot metric comparison bar chart using the project's plot_metric_comparison()\n",
    "from src.utils.tracking import plot_metric_comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if runs:\n",
    "    fig = plot_metric_comparison(runs, metric_name=\"final_rmse\", figsize=(12, 5))\n",
    "    plt.show()\n",
    "\n",
    "    fig = plot_metric_comparison(runs, metric_name=\"final_mae\", figsize=(12, 5))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No runs to plot. Train a model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Export comparison report to HTML\n",
    "from src.utils.tracking import export_comparison_report\n",
    "from pathlib import Path\n",
    "\n",
    "if runs:\n",
    "    report_path = Path(\"../outputs/evaluation/experiment_comparison_report.html\")\n",
    "    export_comparison_report(runs, output_path=report_path, format=\"html\")\n",
    "    print(f\"Report saved to: {report_path.resolve()}\")\n",
    "    print(f\"Open in browser: file://{report_path.resolve()}\")\n",
    "else:\n",
    "    print(\"No runs to export. Train a model first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
